# Knowledge Distillation Configuration
model:
  teacher:
    hidden_layers: [512, 256]
    dropout: 0.2
    activation: "relu"
  
  student:
    hidden_layers: [128]
    dropout: 0.1
    activation: "relu"

training:
  batch_size: 64
  teacher_epochs: 5
  student_epochs: 10
  learning_rate: 0.001
  weight_decay: 1e-4
  
distillation:
  temperature: 3.0
  alpha: 0.7  # Weight between soft and hard loss

data:
  dataset: "MNIST"
  data_dir: "./data"
  download: true
  normalize: true

logging:
  log_level: "INFO"
  log_file: "logs/training.log"
  save_models: true
  model_dir: "models"

visualization:
  save_plots: true
  plot_dir: "plots"
  show_plots: false

device:
  use_cuda: true
  device_id: 0
